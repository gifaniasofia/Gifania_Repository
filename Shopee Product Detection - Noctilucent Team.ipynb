{"cells":[{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport os\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom keras.preprocessing import image\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom tqdm import tqdm","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"TrainImagePaths = []\nfor dirname, _, filenames in os.walk('/kaggle/input/shopee-product-detection-student/train/train/train'):\n    for filename in filenames:\n        if (filename[-3:] == 'jpg'):\n            TrainImagePaths.append(os.path.join(dirname, filename))","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"imgSize = 50\nX_train = []\nY_train = []\nfor imagePath in tqdm(TrainImagePaths):\n    label = imagePath.split(os.path.sep)[-2]\n#     image = cv2.imread(imagePath)\n    image = cv2.imread(imagePath, cv2.IMREAD_GRAYSCALE)\n#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (imgSize, imgSize))\n\n    X_train.append(image)\n    Y_train.append(int(label))\n    \nX_train = np.array(X_train).astype('float16')/255\nY_train = to_categorical(Y_train)","execution_count":4,"outputs":[{"output_type":"stream","text":"100%|██████████| 105390/105390 [10:23<00:00, 168.96it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"to_categorical(int(imagePath.split(os.path.sep)[-2]))","execution_count":74,"outputs":[{"output_type":"execute_result","execution_count":74,"data":{"text/plain":"array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 1.], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = X_train.reshape(105390,imgSize,imgSize,1)\n\nX_train.shape","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"(105390, 50, 50, 1)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv2D(42, kernel_size=(3, 3),activation='relu',input_shape=(imgSize,imgSize,1)))\nmodel.add(Flatten())\nmodel.add(Dense(42, activation='softmax'))","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])\nmodel.fit(X_train, Y_train, epochs=15)","execution_count":7,"outputs":[{"output_type":"stream","text":"Epoch 1/15\n3294/3294 [==============================] - 117s 36ms/step - loss: 3.0996 - accuracy: 0.2136\nEpoch 2/15\n3294/3294 [==============================] - 116s 35ms/step - loss: 2.2932 - accuracy: 0.4034\nEpoch 3/15\n3294/3294 [==============================] - 114s 35ms/step - loss: 1.5351 - accuracy: 0.5961\nEpoch 4/15\n3294/3294 [==============================] - 115s 35ms/step - loss: 0.9571 - accuracy: 0.7524\nEpoch 5/15\n3294/3294 [==============================] - 113s 34ms/step - loss: 0.5966 - accuracy: 0.8524\nEpoch 6/15\n3294/3294 [==============================] - 112s 34ms/step - loss: 0.3904 - accuracy: 0.9087\nEpoch 7/15\n3294/3294 [==============================] - 113s 34ms/step - loss: 0.2746 - accuracy: 0.9397\nEpoch 8/15\n3294/3294 [==============================] - 114s 35ms/step - loss: 0.2040 - accuracy: 0.9576\nEpoch 9/15\n3294/3294 [==============================] - 114s 34ms/step - loss: 0.1588 - accuracy: 0.9694\nEpoch 10/15\n3294/3294 [==============================] - 114s 34ms/step - loss: 0.1309 - accuracy: 0.9750\nEpoch 11/15\n3294/3294 [==============================] - 113s 34ms/step - loss: 0.1127 - accuracy: 0.9795\nEpoch 12/15\n3294/3294 [==============================] - 113s 34ms/step - loss: 0.0956 - accuracy: 0.9829\nEpoch 13/15\n3294/3294 [==============================] - 113s 34ms/step - loss: 0.0846 - accuracy: 0.9863\nEpoch 14/15\n3294/3294 [==============================] - 112s 34ms/step - loss: 0.0743 - accuracy: 0.9880\nEpoch 15/15\n3294/3294 [==============================] - 112s 34ms/step - loss: 0.0670 - accuracy: 0.9894\n","name":"stdout"},{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f240f5266d0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/shopee-product-detection-student/test.csv')\nX_test = []\nfor imageName in tqdm(df['filename']): \n    image = cv2.imread('../input/shopee-product-detection-student/test/test/test/'+imageName, cv2.IMREAD_GRAYSCALE)\n#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (imgSize, imgSize))\n    X_test.append(image)\nX_test = np.array(X_test).astype('float16')/255","execution_count":83,"outputs":[{"output_type":"stream","text":"100%|██████████| 12186/12186 [00:51<00:00, 237.85it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = X_test.reshape(12186,imgSize,imgSize,1)\n\nX_test.shape","execution_count":84,"outputs":[{"output_type":"execute_result","execution_count":84,"data":{"text/plain":"(12186, 50, 50, 1)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"res = model.predict(X_test, batch_size=32)\nres = np.argmax(res, axis=1)\n# print(res)\ndf['category'] = res\ndf['category'] = df.category.apply(lambda c: str(c).zfill(2))\ndf.to_csv('output1-7-3.csv', index = False)","execution_count":85,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"COBA TRAIN"},{"metadata":{"trusted":true},"cell_type":"code","source":"df","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"                                   filename category\n0      fd663cf2b6e1d7b02938c6aaae0a32d2.jpg       12\n1      c7fd77508a8c355eaab0d4e10efd6b15.jpg       27\n2      127f3e6d6e3491b2459812353f33a913.jpg       38\n3      5ca4f2da11eda083064e6c36f37eeb81.jpg       16\n4      46d681a542f2c71be017eef6aae23313.jpg       39\n...                                     ...      ...\n12181  5ba958eacb23cd7d1673bad4dae55784.jpg       10\n12182  efbe41a1c2b666b70e337e438559808b.jpg       19\n12183  79fdaa5ac5ba10dbe8004cabd8c35eb3.jpg       34\n12184  ac3d136124617637a05ba66694e381ef.jpg       31\n12185  7ef61d7cfbad9cfe2db4f64560e3dddd.jpg       26\n\n[12186 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fd663cf2b6e1d7b02938c6aaae0a32d2.jpg</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>c7fd77508a8c355eaab0d4e10efd6b15.jpg</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>127f3e6d6e3491b2459812353f33a913.jpg</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5ca4f2da11eda083064e6c36f37eeb81.jpg</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>46d681a542f2c71be017eef6aae23313.jpg</td>\n      <td>39</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12181</th>\n      <td>5ba958eacb23cd7d1673bad4dae55784.jpg</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>12182</th>\n      <td>efbe41a1c2b666b70e337e438559808b.jpg</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>12183</th>\n      <td>79fdaa5ac5ba10dbe8004cabd8c35eb3.jpg</td>\n      <td>34</td>\n    </tr>\n    <tr>\n      <th>12184</th>\n      <td>ac3d136124617637a05ba66694e381ef.jpg</td>\n      <td>31</td>\n    </tr>\n    <tr>\n      <th>12185</th>\n      <td>7ef61d7cfbad9cfe2db4f64560e3dddd.jpg</td>\n      <td>26</td>\n    </tr>\n  </tbody>\n</table>\n<p>12186 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df = pd.read_csv('../input/shopee-product-detection-student/test.csv')\n# X_test = []\n# for imageName in tqdm(df['filename'][0:10]): \nimage = cv2.imread('../input/shopee-product-detection-student/test/test/test/004316305306b74ecd1c9a887954aa17.jpg', cv2.IMREAD_GRAYSCALE)\n#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nimage = cv2.resize(image, (imgSize, imgSize))\n\nX_test=[]\nX_test.append(image)\nX_test = np.array(X_test).astype('float16')/255\n\nX_test = X_test.reshape(1,imgSize,imgSize,1)\nX_test.shape\n\nres = model.predict(X_test, batch_size=32)\nres = np.argmax(res, axis=1)\n\nprint(res)\n\n# df['category'] = res\n# df['category'] = df.category.apply(lambda c: str(c).zfill(2))\n# df.to_csv('outputcobacoba.csv', index = False)","execution_count":89,"outputs":[{"output_type":"stream","text":"[38]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['filename'][0:10]","execution_count":51,"outputs":[{"output_type":"execute_result","execution_count":51,"data":{"text/plain":"0    fd663cf2b6e1d7b02938c6aaae0a32d2.jpg\n1    c7fd77508a8c355eaab0d4e10efd6b15.jpg\n2    127f3e6d6e3491b2459812353f33a913.jpg\n3    5ca4f2da11eda083064e6c36f37eeb81.jpg\n4    46d681a542f2c71be017eef6aae23313.jpg\n5    f27ffaf1ab259a0efe2c59611de6732b.jpg\n6    5eb8d9688d86ec7277f10fc18f386913.jpg\n7    2ff578b587ec0c086ded573a3a9a9e3a.jpg\n8    936b60783f4f3d3f2b718cc28a1cd629.jpg\n9    2ac0233cc59040fd847f0cd9528772cc.jpg\nName: filename, dtype: object"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}